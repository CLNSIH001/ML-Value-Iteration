states{s1, s2, s3, s4, s5, s6}
values{51.2, 64, 0, 64, 80, 100}
policy{down, down, stay, right, right, up}
number of iterations: 5

////////////////////////////////////////////////////////////////

1.   How many iterations does it take for the Value Iteration algorithm to converge?
     number of iterations: 5
     s1: 51.2, s2: 64, s3: 0, s4: 64, s5: 80, s6: 100

2.   Assuming we start in state s1, give the states that form the optimal policy (π∗) to reach the terminal state (s3).
     state 1 at s1: (0,1) moves down to s4.
     state 4 at s4: (0,0) moves right to s5.
     state 5 at s5: (1,0) moves right to s6.
     state 6 at s6: (2,0) moves up to s3.
     so graphically...
        s1    s2    s3
        |            ^
        |            |
        s4 -> s5 -> s6

3.   Is it possible to change the reward function so that V∗ changes, but the optimal policy (π∗) remains unchanged?
     Yes it is possible. When I mmultiplied the reward funtion for each transition by a factor of 3 and ran the code, I got new values for each state 
     but the policy remain unchanged. Same thing happened when I changed every 0 reward to 5 instead of multiplying everything by 3.
     values{153.6, 192, 0, 192, 240, 300}
     policy{down, down, , right, right, up}
     values{63.4, 73, 0, 73, 85, 100}
     policy{down, down, , right, right, up}
     When I changed the rewards for each transition between states to any number (view commented out parts of reward function),
     the policies, values and even iterations changed:
     values{35, 35, 0, 35, 35, 35}
     policy{down, left, , up, left, left}
     number of iterations: 157
